Some explaination to the accept paper, hope the reader will satsify:
1.Misinterpret of "cross-encoder" and "dense retrieval". As someone notice that the attention definition in the Section 4 is according to cross-encoder structrue but not bi-encoder struture. This is not the same as that we claimed in the anstract and introduction that "we explore the differetion between the sparse retrieval and dense retrieval". Here are two explanation:
  * We find that the cross encoder perform better then dense retrieval. But some times the dense models perform better, such as in COLIEE competition. Besides, We have inspected the performamce of different type of retrieval model(sparse models including BM25,QL,TF-IDF representation and neural models including DPR, mono-bert, sentence-level interaction) on the dataset in terms of each query. We find that the models in each class have higher spearman correlation then those between two class. We guess the models in the same class have the similar mechnisms thus the model attention will be similar. At least this is proven for models of lexical and cross-encoder.
  * The definition of the attention in bi-encoder is not so reliable. As the dot production of the dense vector do not directly reflect the interaction between query tokens and document tokens. One possible solution is to dive into the gradient, but the gradient from the dot product to the token embedding do not as intuitive as the attention in cross encoder.
  * We tried to leverage the learned sparse retrieval or sparse auto-encoder to interpret the nerual models. But It do not perform well here.  
So the cross-encoder we use here may be a representative for nerual models. The cross-encoder vs lexical model seems not good comparision. I have to admit that we want to enphasis the model attention between lexical model and neural model is different. Hopefully the reader ignore this error.
2. The metric of NDCG. The measure metric in the paper is according to the original LeCaRD paper. You may notice the NDCG metric is much higher the standard test, this because the LeCaRD levelerage the ensambled top-30 recalled documents to calculate the IDCG, which make the idcg lower.
3. The latency problem. The extraction or summarization depends on the API or inference efficiency of open source LLMs.
  * In the training time, the content selection is offline.
  * In the inference time, the content selection is online.
